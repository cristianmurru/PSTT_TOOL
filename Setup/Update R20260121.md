# Update R20260121

- Aggiornamento relativo all'integrazione completa Kafka per pubblicazione messaggi su topic da schedulazioni
- **FEATURE**: Integrazione completa Kafka con modelli, servizi, API, dashboard UI e test
- Supporto multi-cluster Kafka tramite `connections.json`
- Publishing asincrono batch e singolo con retry automatico
- Metrics tracking e health monitoring
- Nessuna breaking change: tutte le funzionalitÃ  esistenti rimangono intatte

## Prerequisiti
- Accesso al server con privilegi amministrativi
- Finestra di manutenzione concordata (utente non operativo durante l'intervento)
- Percorso di installazione corrente dell'app (es.: `C:\PSTT_TOOL`)
- Nome del Servizio Windows dell'applicazione: `PSTT_Tool`
- Pacchetto sorgente aggiornato
- **Nuovo**: Kafka brokers configurati e raggiungibili (opzionale - solo se si usa la funzionalitÃ  Kafka)

## 1) Stop del Servizio

Scegli una delle opzioni:

- Script di gestione (se presente nella root):
```powershell
# Eseguire nella cartella di installazione
.\manage_service.ps1 stop
```

- PowerShell nativo:
```powershell
Stop-Service -Name PSTT_Tool -Force
# Verifica stato
Get-Service -Name PSTT_Tool
```

- SC.exe:
```cmd
sc stop PSTT_Tool
sc query PSTT_Tool
```

## 2) Backup installazione corrente

Creare una copia di sicurezza dell'installazione:
```powershell
$src = "C:\PSTT_TOOL"
$dst = "C:\Backup\PSTT_TOOL_2026-01-21"
New-Item -ItemType Directory -Path $dst -Force | Out-Null
robocopy $src $dst /MIR /R:2 /W:2
```

## 3) Sovrascrittura file aggiornati (esclusioni)

- Copiare i file aggiornati dal pacchetto sorgente nella cartella di installazione
- **Non sovrascrivere**:
  - `connections.json`
  - `.env`
  - `exports/scheduler_history.json`
  - `exports/scheduler_metrics.json`
  - `exports/kafka_metrics.json` (nuovo, verrÃ  creato automaticamente)

Suggerimento: se usi `robocopy`, escludi esplicitamente:
```powershell
robocopy "C:\Pacchetto\PSTT_TOOL" "C:\PSTT_TOOL" /MIR /R:2 /W:2 /XD .venv exports logs /XF connections.json .env
```

### File modificati/aggiunti in R20260121

#### ðŸ†• Nuovi File - Integrazione Kafka

**Modelli**:
- **`app/models/kafka.py`** âœ… (nuovo)
  - `KafkaConnectionConfig`: Configurazione connessione multi-cluster
  - `KafkaProducerConfig`: Configurazione producer (acks, batch size, compression)
  - `KafkaExportConfig`: Configurazione export schedulazioni
  - `KafkaMetrics`: Tracking metriche throughput/latency
  - `BatchResult`: Risultati batch publish
  - `KafkaHealthStatus`: Health check status

**Servizi**:
- **`app/services/kafka_service.py`** âœ… (nuovo)
  - Connection management asincrona con security (PLAINTEXT/SSL/SASL)
  - Message publishing singolo con retry automatico
  - Batch publishing con chunking intelligente (default 100 msg/chunk)
  - JSON serialization custom per datetime/date/Decimal
  - Health check con verifica connettivitÃ  broker
  - Context manager per gestione automatica risorse
  
- **`app/services/kafka_metrics_service.py`** âœ… (nuovo)
  - Tracciamento temporale metriche con granularitÃ  oraria
  - Aggregazione multi-dimensionale (topic, connection, total)
  - Export JSON persistente su disco (`exports/kafka_metrics.json`)
  - Cleanup automatico metriche vecchie

**API**:
- **`app/api/kafka.py`** âœ… (nuovo)
  - `GET /api/kafka/connections`: Elenco connessioni Kafka configurate
  - `POST /api/kafka/test`: Test connessione broker
  - `POST /api/kafka/send`: Invio messaggio singolo
  - `POST /api/kafka/batch`: Invio batch messaggi
  - `GET /api/kafka/metrics/summary`: Metriche globali aggregate
  - `GET /api/kafka/metrics/hourly`: Metriche ultime 24h
  - `GET /api/kafka/metrics/topics`: Breakdown per topic
  - `GET /api/kafka/health`: Health check producer

**Dashboard UI**:
- **`app/templates/kafka_dashboard.html`** âœ… (nuovo)
  - Overview metriche real-time (last 24h)
  - Grafici throughput e latency con Chart.js
  - Breakdown per topic con tabelle interattive
  - Sezione test: invio messaggi singoli/batch da UI
  - Health status producer con indicatori visivi

**Test**:
- **`tests/test_kafka_config.py`** âœ… (nuovo) - 25 test configurazione
- **`tests/test_kafka_service.py`** âœ… (nuovo) - 40 test servizio
- **`tests/test_kafka_metrics_service.py`** âœ… (nuovo) - 16 test metriche
- **`tests/test_scheduler_kafka_integration.py`** âœ… (nuovo) - 15 test integrazione scheduler
- **`tests/test_api_kafka.py`** âœ… (nuovo) - 15 test API endpoints

**Tools**:
- **`tools/kafka_benchmark.py`** âœ… (nuovo)
  - 3 modalitÃ  test: single, batch, mixed load
  - Metriche latenza: avg, p50, p90, p99
  - Calcolo throughput (msg/sec)
  - Command-line interface con argparse

**Documentazione**:
- **`docs/KAFKA_SETUP.md`** âœ… (nuovo, ~400 righe)
  - Setup guide completa
  - Troubleshooting
  - Performance tuning
  - Security SSL/SASL
  - FAQ
  
- **`docs/KAFKA_RUNBOOK.md`** âœ… (nuovo, ~600 righe)
  - Operational procedures
  - Emergency scenarios
  - Health checks
  - Escalation matrix

- **`docs/KAFKA_CONFIGURATION_EXAMPLES.md`** âœ… (nuovo)
  - Esempi configurazione per scenari comuni
  - Security patterns
  - Multi-cluster setup

#### ðŸ“ File Modificati - Integrazione Kafka

**Backend Core**:
- **`app/core/config.py`**
  - âœ… Aggiunte 36 variabili configurazione Kafka con prefisso `KAFKA_`
  - âœ… Helper `get_kafka_config()` per caricamento configurazione da connections.json
  - âœ… Supporto multi-cluster tramite sezione `kafka_connections` in connections.json

- **`app/models/scheduling.py`**
  - âœ… Enum `SharingMode`: aggiunto valore `KAFKA`
  - âœ… Nuovi campi opzionali: `kafka_topic`, `kafka_key_field`, `kafka_batch_size`, `kafka_include_metadata`
  - âœ… Validazione topic Kafka (no spazi, no caratteri speciali)

- **`app/services/scheduler_service.py`**
  - âœ… Metodo `_execute_kafka_export()` per export verso Kafka
  - âœ… Pipeline: query â†’ trasformazione â†’ batch export
  - âœ… Metadata automatici: source_query, source_connection, export_timestamp, export_id
  - âœ… Tracking completo in `scheduler_history.json`: kafka_topic, kafka_messages_sent, kafka_messages_failed, kafka_duration_sec

- **`app/main.py`**
  - âœ… Registrazione router Kafka: `app.include_router(kafka_router, prefix="/api/kafka", tags=["kafka"])`
  - âœ… Import nuovi moduli Kafka

**Frontend**:
- **`app/frontend/scheduler_dashboard.html`**
  - âœ… Dropdown "Condivisione": aggiunta opzione "Kafka"
  - âœ… 4 nuovi campi specifici Kafka (topic, key_field, batch_size, include_metadata)
  - âœ… Show/hide dinamico campi in base a modalitÃ  selezionata
  - âœ… Form submission con costruzione automatica `kafka_config` object

- **`app/templates/index.html`**
  - âœ… Aggiunto link menu "Kafka Dashboard" per accesso nuovo dashboard

- **`app/frontend/settings.html`**
  - âœ… Link di navigazione verso Kafka Dashboard

- **`app/frontend/logs.html`**
  - âœ… Link di navigazione verso Kafka Dashboard

**Dipendenze**:
- **`requirements.txt`**
  - âœ… `kafka-python-ng==2.2.2` (producer Kafka)
  - âœ… `aiokafka==0.10.0` (async Kafka per operazioni non-blocking)

**Documentazione Generale**:
- **`docs/README.md`**
  - âœ… Aggiunta sezione Integrazione Kafka
  - âœ… Link documentazione Kafka

- **`docs/CHANGELOG.md`**
  - âœ… Versione 1.1.0 con dettaglio completo modifiche (STEP 1-7)

## 4) Modifiche a `connections.json`

### 4.1 Aggiungere sezione Kafka (opzionale)

Se si desidera utilizzare la funzionalitÃ  Kafka, aggiungere la sezione `kafka_connections` al file `connections.json`:

```json
{
  "databases": [
    ... esistenti ...
  ],
  "kafka_connections": [
    {
      "name": "kafka-dev",
      "display_name": "Kafka Development",
      "bootstrap_servers": ["localhost:9092"],
      "security_protocol": "PLAINTEXT",
      "description": "Kafka development cluster"
    },
    {
      "name": "kafka-prod",
      "display_name": "Kafka Production",
      "bootstrap_servers": ["kafka1.prod:9093", "kafka2.prod:9093"],
      "security_protocol": "SASL_SSL",
      "sasl_mechanism": "SCRAM-SHA-512",
      "sasl_username": "pstt_user",
      "sasl_password": "${KAFKA_PROD_PASSWORD}",
      "ssl_cafile": "C:/certs/kafka-ca.pem",
      "ssl_certfile": "C:/certs/kafka-client.pem",
      "ssl_keyfile": "C:/certs/kafka-client-key.pem",
      "description": "Kafka production cluster with SSL/SASL"
    }
  ]
}
```

**Note**:
- Se la sezione `kafka_connections` non Ã¨ presente, l'applicazione funzionerÃ  normalmente senza funzionalitÃ  Kafka
- I campi `sasl_username` e `sasl_password` supportano variabili d'ambiente tramite sintassi `${VAR_NAME}`
- Per security_protocol sono supportati: `PLAINTEXT`, `SSL`, `SASL_PLAINTEXT`, `SASL_SSL`
- Per sasl_mechanism sono supportati: `PLAIN`, `SCRAM-SHA-256`, `SCRAM-SHA-512`

### 4.2 `.env`

Aggiungere variabili opzionali per configurazione Kafka avanzata (valori di default giÃ  configurati):

```env
# === Kafka Configuration (Opzionale) ===
# Timeout connessione broker (ms)
KAFKA_TIMEOUT_MS=30000

# Dimensione batch default per export
KAFKA_BATCH_SIZE=100

# Soglia success rate per retry (%)
KAFKA_SUCCESS_THRESHOLD=95.0

# Tipo compressione messaggi (none|gzip|snappy|lz4|zstd)
KAFKA_COMPRESSION_TYPE=snappy

# ModalitÃ  acknowledgment (-1=all|0=none|1=leader)
KAFKA_ACKS=all

# Abilita idempotence producer
KAFKA_ENABLE_IDEMPOTENCE=true

# Password Kafka production (se necessaria)
KAFKA_PROD_PASSWORD=your_secure_password_here
```

**Note**:
- Tutte queste variabili sono opzionali e hanno valori di default ragionevoli
- Modificare solo se necessario per requisiti specifici ambiente

## 5) Verifica installazione

Dopo il restart, verificare:

1. **Servizio avviato correttamente**:
```powershell
Get-Service -Name PSTT_Tool
# Stato: Running
```

2. **Log applicazione**:
```powershell
# Controllare ultimi log per errori
Get-Content "C:\PSTT_TOOL\logs\app_*.log" -Tail 50
```

3. **Accesso dashboard Kafka** (se configurato):
   - Navigare a: http://localhost:8000/kafka
   - Verificare presenza connessioni Kafka configurate
   - Testare connessione cliccando "Test Connection"

4. **Test funzionalitÃ  esistenti**:
   - Homepage: http://localhost:8000/
   - Scheduler: http://localhost:8000/scheduler
   - Settings: http://localhost:8000/settings
   - Logs: http://localhost:8000/logs

## 6) Test suite completa

L'applicazione include una suite completa di 196 test automatici. Per eseguirla:

```powershell
# Attivare virtual environment
.venv\Scripts\Activate.ps1

# Eseguire test
python -m pytest tests/ -v

# Atteso: 196 passed
```

## 7) Rollback (se necessario)

In caso di problemi, ripristinare il backup:

```powershell
# Stop servizio
Stop-Service -Name PSTT_Tool -Force

# Ripristina backup
robocopy "C:\Backup\PSTT_TOOL_2026-01-21" "C:\PSTT_TOOL" /MIR /R:2 /W:2

# Start servizio
Start-Service -Name PSTT_Tool

# Verifica
Get-Service -Name PSTT_Tool
```

## 8) Note aggiuntive

### CompatibilitÃ 
- **Zero breaking changes**: L'integrazione Kafka Ã¨ completamente opzionale
- Tutte le funzionalitÃ  esistenti (Filesystem, Email) continuano a funzionare come prima
- Se `kafka_connections` non Ã¨ presente in `connections.json`, Kafka non viene inizializzato

### Performance
- Throughput testato: >100 msg/sec con batch da 1000 messaggi
- Latenza media: <50ms per messaggio singolo (rete locale)
- Retry automatico con backoff esponenziale: 100ms â†’ 200ms â†’ 400ms

### Security
- Supporto completo SSL/TLS per comunicazione criptata
- Supporto SASL per autenticazione (PLAIN, SCRAM-SHA-256, SCRAM-SHA-512)
- Idempotence producer abilitata per prevenire duplicati

### Monitoring
- Metriche persistenti in `exports/kafka_metrics.json`
- Dashboard real-time con grafici throughput/latency
- Health check endpoint: `GET /api/kafka/health`
- Cleanup automatico metriche >30 giorni

## 9) Riferimenti documentazione

Consultare i seguenti documenti per informazioni dettagliate:

- **Setup Kafka**: `docs/KAFKA_SETUP.md`
- **Runbook operativo**: `docs/KAFKA_RUNBOOK.md`
- **Esempi configurazione**: `docs/KAFKA_CONFIGURATION_EXAMPLES.md`
- **Changelog completo**: `docs/CHANGELOG.md`
- **README aggiornato**: `docs/README.md`

## 10) Supporto

Per problemi o domande:
1. Consultare `docs/KAFKA_SETUP.md` sezione Troubleshooting
2. Verificare log applicazione in `logs/`
3. Verificare metriche Kafka in `exports/kafka_metrics.json`
4. Testare connessione broker con dashboard Kafka

---

**Versione**: 1.1.0  
**Data**: 2026-01-21  
**Branch**: feature/R20260117  
**Responsabile**: Development Team PSTT
